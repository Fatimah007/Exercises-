{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fatimah007/Exercises-/blob/main/Copy_of_web_scrape_exercise_e.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "274661b6",
      "metadata": {
        "id": "274661b6"
      },
      "source": [
        "# Web Scraping Tutorial\n",
        "\n",
        "This notebook provides a step-by-step guide to scrape data from a website. Web scraping is a technique used to extract information from websites by transforming the data on web pages into a structured format. This is particularly useful for data analysis, machine learning, and other data-driven tasks.\n",
        "\n",
        "In this tutorial, we will walk through the process of scraping product information from a sample e-commerce site. By following these steps, you will learn how to:\n",
        "\n",
        "1. Send HTTP requests to retrieve web pages.\n",
        "2. Parse HTML content using BeautifulSoup.\n",
        "3. Identify and extract relevant data elements from the parsed HTML.\n",
        "4. Store the extracted data in a structured format using pandas.\n",
        "5. Save the data to a CSV file.\n",
        "6. Optionally, save the data to a database such as MongoDB.\n",
        "\n",
        "The website we will be scraping is [ScrapeMe](https://scrapeme.live/shop/). This site is designed for practice purposes and contains a variety of products with details such as names and prices, which makes it an ideal candidate for learning web scraping techniques.\n",
        "\n",
        "Before you begin, please visit the site to understand its structure. This will help you identify the elements you need to scrape.\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18a87e75",
      "metadata": {
        "id": "18a87e75"
      },
      "source": [
        "## Import libraries here"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "-VQ4kHxAR_e-"
      },
      "id": "-VQ4kHxAR_e-",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5bdedab8",
      "metadata": {
        "id": "5bdedab8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "6cf5a5ab-11f4-41e4-ba44-4490a221db88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.7.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests\n",
        "!pip install beautifulsoup4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7f9f606",
      "metadata": {
        "id": "c7f9f606"
      },
      "source": [
        "## Step 1: Send a request to the website"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "8f4e3051",
      "metadata": {
        "id": "8f4e3051",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de0b0848-6fc1-4d97-d885-a442b187e08a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200\n",
            "\n",
            "<!doctype html>\n",
            "<html lang=\"en-GB\">\n",
            "<head>\n",
            "<meta charset=\"UTF-8\">\n",
            "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1, maximum-scale=2.0\">\n",
            "<link rel=\"profile\" href=\"http://gmpg.org/xfn/11\">\n",
            "<link rel=\"pingback\" href=\"https://scrapeme.live/xmlrpc.php\">\n",
            "\n",
            "<title>Products &#8211; ScrapeMe</title>\n",
            "<link rel='dns-prefetch' href='//fonts.googleapis.com' />\n",
            "<link rel='dns-prefetch' href='//s.w.org' />\n",
            "<link rel=\"alternate\" type=\"application/rss+xml\" title=\"ScrapeMe &raquo; Feed\" href=\"ht\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "# URL of the web page you want to scrape\n",
        "url = 'https://scrapeme.live/shop/'\n",
        "\n",
        "# Fetch the content of the web page\n",
        "response = requests.get(url)\n",
        "\n",
        "# Print the status code to check if the request was successful\n",
        "print(response.status_code)\n",
        "\n",
        "# Print the first 500 characters of the content\n",
        "print(response.text[:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71d1d25f",
      "metadata": {
        "id": "71d1d25f"
      },
      "source": [
        "## Step 2: Parse the HTML content of the page"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e2630791",
      "metadata": {
        "id": "e2630791",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6491499a-b608-4c24-c9b0-00aceee0b85c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<title>Products – ScrapeMe</title>\n",
            "Just another WordPress site\n",
            "\n",
            "\tShowing 1–16 of 755 results\n",
            "\n",
            "\tShowing 1–16 of 755 results\n"
          ]
        }
      ],
      "source": [
        "#send http requests to retrieve web pages\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Create a BeautifulSoup object and specify the parser\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Print the title of the web page\n",
        "print(soup.title)\n",
        "\n",
        "# Print all paragraph tags\n",
        "for p in soup.find_all('p'):\n",
        "    print(p.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "136215c4",
      "metadata": {
        "id": "136215c4"
      },
      "source": [
        "## Step 3: Inspect the website and identify the elements to scrape\n",
        "Inspect the website and identify the elements (e.g., product names, prices, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "162008b8",
      "metadata": {
        "id": "162008b8"
      },
      "outputs": [],
      "source": [
        "# Extract all links from the web page\n",
        "links = soup.find_all('product names','prices')\n",
        "\n",
        "# Print the URLs of the links\n",
        "for link in links:\n",
        "    print(link.get('href'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64e51da7",
      "metadata": {
        "id": "64e51da7"
      },
      "source": [
        "## Step 4: Extract the desired data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "06b9477d",
      "metadata": {
        "id": "06b9477d"
      },
      "outputs": [],
      "source": [
        "with open('links.txt', 'w') as file:\n",
        "    for link in links:\n",
        "        href = link.get('href')\n",
        "        if href is not None:\n",
        "            file.write(href + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extract specific data\n",
        "\n",
        "for item in soup.find_all('img'):\n",
        "     imgd=item['src']\n",
        "     print(imgd)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rwICb2ljSgMM",
        "outputId": "ecdb38d5-fdbe-4449-8283-eee38f444050"
      },
      "id": "rwICb2ljSgMM",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://scrapeme.live/wp-content/uploads/2018/08/001-350x350.png\n",
            "https://scrapeme.live/wp-content/uploads/2018/08/002-350x350.png\n",
            "https://scrapeme.live/wp-content/uploads/2018/08/003-350x350.png\n",
            "https://scrapeme.live/wp-content/uploads/2018/08/004-350x350.png\n",
            "https://scrapeme.live/wp-content/uploads/2018/08/005-350x350.png\n",
            "https://scrapeme.live/wp-content/uploads/2018/08/006-350x350.png\n",
            "https://scrapeme.live/wp-content/uploads/2018/08/007-350x350.png\n",
            "https://scrapeme.live/wp-content/uploads/2018/08/008-350x350.png\n",
            "https://scrapeme.live/wp-content/uploads/2018/08/009-350x350.png\n",
            "https://scrapeme.live/wp-content/uploads/2018/08/010-350x350.png\n",
            "https://scrapeme.live/wp-content/uploads/2018/08/011-350x350.png\n",
            "https://scrapeme.live/wp-content/uploads/2018/08/012-350x350.png\n",
            "https://scrapeme.live/wp-content/uploads/2018/08/013-350x350.png\n",
            "https://scrapeme.live/wp-content/uploads/2018/08/014-350x350.png\n",
            "https://scrapeme.live/wp-content/uploads/2018/08/015-350x350.png\n",
            "https://scrapeme.live/wp-content/uploads/2018/08/016-350x350.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scrap_data =[{'text':item['src'],'href':item.get('href')} for item in soup.find_all('img')]"
      ],
      "metadata": {
        "id": "h8DvrE3WabVh"
      },
      "id": "h8DvrE3WabVh",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ebe02982",
      "metadata": {
        "id": "ebe02982"
      },
      "source": [
        "## Step 5: Create a DataFrame to store the extracted data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "df0dfe22",
      "metadata": {
        "id": "df0dfe22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "14c15dec-0d82-4ad4-cc89-af567c8de162"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                 text  href\n",
            "0   https://scrapeme.live/wp-content/uploads/2018/...  None\n",
            "1   https://scrapeme.live/wp-content/uploads/2018/...  None\n",
            "2   https://scrapeme.live/wp-content/uploads/2018/...  None\n",
            "3   https://scrapeme.live/wp-content/uploads/2018/...  None\n",
            "4   https://scrapeme.live/wp-content/uploads/2018/...  None\n",
            "5   https://scrapeme.live/wp-content/uploads/2018/...  None\n",
            "6   https://scrapeme.live/wp-content/uploads/2018/...  None\n",
            "7   https://scrapeme.live/wp-content/uploads/2018/...  None\n",
            "8   https://scrapeme.live/wp-content/uploads/2018/...  None\n",
            "9   https://scrapeme.live/wp-content/uploads/2018/...  None\n",
            "10  https://scrapeme.live/wp-content/uploads/2018/...  None\n",
            "11  https://scrapeme.live/wp-content/uploads/2018/...  None\n",
            "12  https://scrapeme.live/wp-content/uploads/2018/...  None\n",
            "13  https://scrapeme.live/wp-content/uploads/2018/...  None\n",
            "14  https://scrapeme.live/wp-content/uploads/2018/...  None\n",
            "15  https://scrapeme.live/wp-content/uploads/2018/...  None\n"
          ]
        }
      ],
      "source": [
        "df=pd.DataFrame(scrap_data)\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7fb0de6",
      "metadata": {
        "id": "a7fb0de6"
      },
      "source": [
        "## Step 6: Save the data to a CSV file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "24e44e64",
      "metadata": {
        "id": "24e44e64"
      },
      "outputs": [],
      "source": [
        "df.to_csv('scrap_data.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee480802",
      "metadata": {
        "id": "ee480802"
      },
      "source": [
        "## Step 7: Print the DataFrame to verify the extracted data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "bee5d07c",
      "metadata": {
        "id": "bee5d07c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "33f4ae63-6a19-481d-db8a-842079588394"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                 text  href\n",
            "0   https://scrapeme.live/wp-content/uploads/2018/...  None\n",
            "1   https://scrapeme.live/wp-content/uploads/2018/...  None\n",
            "2   https://scrapeme.live/wp-content/uploads/2018/...  None\n",
            "3   https://scrapeme.live/wp-content/uploads/2018/...  None\n",
            "4   https://scrapeme.live/wp-content/uploads/2018/...  None\n",
            "5   https://scrapeme.live/wp-content/uploads/2018/...  None\n",
            "6   https://scrapeme.live/wp-content/uploads/2018/...  None\n",
            "7   https://scrapeme.live/wp-content/uploads/2018/...  None\n",
            "8   https://scrapeme.live/wp-content/uploads/2018/...  None\n",
            "9   https://scrapeme.live/wp-content/uploads/2018/...  None\n",
            "10  https://scrapeme.live/wp-content/uploads/2018/...  None\n",
            "11  https://scrapeme.live/wp-content/uploads/2018/...  None\n",
            "12  https://scrapeme.live/wp-content/uploads/2018/...  None\n",
            "13  https://scrapeme.live/wp-content/uploads/2018/...  None\n",
            "14  https://scrapeme.live/wp-content/uploads/2018/...  None\n",
            "15  https://scrapeme.live/wp-content/uploads/2018/...  None\n"
          ]
        }
      ],
      "source": [
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea5dc916",
      "metadata": {
        "id": "ea5dc916"
      },
      "source": [
        "## Step 8: Save the data to a database of your choice. If you are using MongoDB, include the code here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec9b8a29",
      "metadata": {
        "id": "ec9b8a29"
      },
      "outputs": [],
      "source": [
        "#"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}